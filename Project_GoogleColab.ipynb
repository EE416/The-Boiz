{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Project.ipynb","provenance":[{"file_id":"https://github.com/EE416/The-Boiz/blob/main/Project.ipynb","timestamp":1606044655949}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uHLl1yPMplue","executionInfo":{"status":"ok","timestamp":1606159603736,"user_tz":600,"elapsed":4905,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["import random\n","import csv\n","import os\n","import os.path\n","import shutil\n","import cv2\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm # Displays a progress bar\n","\n","import torch\n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","from torch.utils.data import Dataset, Subset, DataLoader, random_split\n","\n","import pandas as pd\n","import h5py"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"WF_DuRGJqmfo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606159685451,"user_tz":600,"elapsed":83614,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}},"outputId":"3821940f-dfd0-4267-dd1b-7101d97fad6c"},"source":["# from google.colab import drive\n","# drive.mount('/content/drive')\n","\n","!gdown https://drive.google.com/uc?id=1olKESqgAm5GeAmuRC21BpTBkxFrgmUIp\n","!unzip /content/Data"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1olKESqgAm5GeAmuRC21BpTBkxFrgmUIp\n","To: /content/Data.zip\n","977MB [00:04, 210MB/s]\n","Archive:  /content/Data.zip\n","   creating: Data/Test/\n","   creating: Data/Test/Images/\n","  inflating: Data/Test/Images/imagedata.hdf5  \n","   creating: Data/Test/Labels/\n","  inflating: Data/Test/Labels/labeldata.hdf5  \n","   creating: Data/Train/\n","   creating: Data/Train/Images/\n","  inflating: Data/Train/Images/imagedata.hdf5  \n","   creating: Data/Train/Labels/\n","  inflating: Data/Train/Labels/labeldata.hdf5  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"KpjDNBt5-aqu","executionInfo":{"status":"ok","timestamp":1606159695901,"user_tz":600,"elapsed":80966,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["class DataNonNorm:\n","    def __init__(self, root):\n","        self.ROOT = root\n","        self.images = self.read_images(os.path.join(root, \"Images\"))\n","        self.labels = self.read_labels(os.path.join(root, \"Labels\"))\n","\n","    def __len__(self):\n","        # Return number of points in the dataset\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Here we have to return the item requested by `idx`. The PyTorch DataLoader class will use this method to make an iterable for training/validation loop.\n","        img = self.images[idx] / 255\n","        #label = self.labels[idx]\n","        return img\n","  \n","    # Read Images\n","    def read_images(self, path:str) -> list:\n","        imgs = []\n","        with h5py.File(os.path.join(path, \"imagedata.hdf5\"), \"r\") as f:\n","            for i in sorted(f.keys(), key = int ):\n","                imgs.append(np.array(f.get(str(i))))\n","        #output = torch.tensor(output)\n","        return imgs\n","      \n","    # Read Labels\n","    def read_labels(self, path:str) -> list:\n","        output = []\n","        with h5py.File(os.path.join(path, \"labeldata.hdf5\"), \"r\") as f:\n","            labels = f[\"labels\"][()]\n","        return labels       \n","\n","# # Load the dataset and train and test splits\n","# print(\"Loading datasets...\")\n","\n","# # Data path\n","dataTrain = DataNonNorm(os.path.normpath('./Data/Train'))\n","dataTest  = DataNonNorm(os.path.normpath('./Data/Test'))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"IRne1h7R_oQg","executionInfo":{"status":"ok","timestamp":1606159695907,"user_tz":600,"elapsed":78510,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["trainloader = DataLoader(dataTrain, batch_size=32, shuffle=True)\n","testloader = DataLoader(dataTest, batch_size=2, shuffle=True)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVcYbvAO-mk9","executionInfo":{"status":"ok","timestamp":1606159726233,"user_tz":600,"elapsed":100731,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}},"outputId":"64784ef6-4a5a-4be8-ddbb-797db4bc3818"},"source":["import math\n","# Get mean and std deviation of training and test\n","tnMean = 0.\n","tnStd = 0.\n","count = 0.\n","var = 0.\n","for data in trainloader:\n","    tnMean += np.mean(np.array(data))\n","    var += np.var(np.array(data))\n","    count += 1\n","\n","tnMean /= count\n","tnStd = math.sqrt(var / count)\n","\n","print(tnMean)\n","print(tnStd)\n","\n","tstMean = 0.\n","tstStd = 0.\n","count = 0.\n","var = 0.\n","for data in testloader:\n","    tstMean += np.mean(np.array(data))\n","    var += np.var(np.array(data))\n","    count += 1\n","\n","tstMean /= count\n","tstStd = math.sqrt(var / count)\n","\n","print(tstMean)\n","print(tstStd)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["0.6535338665393978\n","0.2691491382000663\n","0.6536001114379154\n","0.26438978120602336\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_Fv-Ef45jn7n","executionInfo":{"status":"ok","timestamp":1606164327201,"user_tz":600,"elapsed":543,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["params = {\n","  \"tnMean\": tnMean,\n","  \"tstMean\": tstMean,\n","  \"tnStd\": tnStd,\n","  \"tstStd\": tstStd,\n","  \"batch\": 32,\n","  \"l1Tol2Features\": 25,\n","  \"epochs\": 30,\n","  \"lr\": 0.001,\n","  \"wtDecay\": 0.004,\n","  \"frozenChildren\": 5\n","}"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"IBy7YEltplue","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606159931122,"user_tz":600,"elapsed":6180,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}},"outputId":"9c90f641-ff28-4a42-ed50-324f4b19f85a"},"source":["class Data:\n","\n","    def __init__(self, root, mn, std):\n","        self.ROOT = root\n","        self.images = self.read_images(os.path.join(root, \"Images\"))\n","        self.labels = self.read_labels(os.path.join(root, \"Labels\"))\n","\n","        self.transform = MyTransform = transforms.Compose([\n","            transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n","            transforms.Normalize((mn, mn, mn), (std, std, std)) # TODO: Normalize to zero mean and unit variance with appropriate parameters 0.5\n","            ])\n","\n","    def __len__(self):\n","        # Return number of points in the dataset\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        # Here we have to return the item requested by `idx`. The PyTorch DataLoader class will use this method to make an iterable for training/validation loop.\n","        img = self.transform(self.images[idx])\n","        # img = self.images[idx] / 255\n","        label = self.labels[idx]\n","        return img, label\n","  \n","    # Read Images\n","    def read_images(self, path:str) -> list:\n","        imgs = []\n","        with h5py.File(os.path.join(path, \"imagedata.hdf5\"), \"r\") as f:\n","            for i in sorted(f.keys(), key = int ):\n","                imgs.append(np.array(f.get(str(i))))\n","        #output = torch.tensor(output)\n","        return imgs\n","      \n","    # Read Labels\n","    def read_labels(self, path:str) -> list:\n","        output = []\n","        with h5py.File(os.path.join(path, \"labeldata.hdf5\"), \"r\") as f:\n","            labels = f[\"labels\"][()]\n","        return labels       \n","\n","# Load the dataset and train and test splits\n","print(\"Loading datasets...\")\n","\n","# Data path\n","dataTrain = Data(os.path.normpath('./Data/Train'), tnMean, tnStd)\n","dataTest  = Data(os.path.normpath('./Data/Test'),tstMean, tstStd)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Loading datasets...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wCnEOl_0plue"},"source":["print(dataTrain.__len__())\n","print(len(dataTrain.labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OmzaFVlM-at7"},"source":["from PIL import Image\n","\n","with h5py.File(os.path.join('./Data/Train/Images', \"imagedata.hdf5\"), \"r\") as f:\n","  img = Image.fromarray(np.array(f.get(\"1\")))\n","display(img)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Z5bXQC2plue","executionInfo":{"status":"ok","timestamp":1606159946418,"user_tz":600,"elapsed":545,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["# print(\"Done!\")\n","\n","# # Create dataloaders\n","# # TODO: Experiment with different batch sizes\n","trainloader = DataLoader(dataTrain, batch_size=params[\"batch\"], shuffle=True, drop_last=True)\n","testloader = DataLoader(dataTest, batch_size=32, shuffle=True, drop_last=True)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"yHdKPym7plue","executionInfo":{"status":"ok","timestamp":1606159948164,"user_tz":600,"elapsed":572,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["from torchvision import datasets, transforms\n","import torchvision.models as models"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"32EIpR-Xplue","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606167887386,"user_tz":600,"elapsed":1707900,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}},"outputId":"e26815f1-e7d6-42fb-baa2-30ae2cf1357c"},"source":["class Network(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        # TODO: [Transfer learning with pre-trained ResNet-50] 1) Define how many first layers of convolutoinal neural network (CNN) feature extractor in ResNet-50 to be \"frozen\" and 2) design your own fully-connected network (FCN) classifier.\n","        # 1) You will only refine last several layers of CNN feature extractor in ResNet-50 that mainly relate to high-level vision task. Determine how many first layers of ResNet-50 should be frozen to achieve best performances. Commented codes below will help you understand the architecture, i.e., \"children\", of ResNet-50.\n","        # 2) Design your own FCN classifier. Here I provide a sample of two-layer FCN.\n","        # Refer to PyTorch documentations of torch.nn to pick your layers. (https://pytorch.org/docs/stable/nn.html)\n","        # Some common Choices are: Linear, ReLU, Dropout, MaxPool2d, AvgPool2d\n","        # If you have many layers, consider using nn.Sequential() to simplify your code\n","        \n","        # Load pretrained ResNet-50\n","        self.model_resnet = models.resnet50(pretrained=True)\n","        \n","        # The code below can show children of ResNet-50\n","        # child_counter = 0\n","        # for child in model.children():\n","        #    print(\" child\", child_counter, \"is -\")\n","        #    print(child)\n","        #    child_counter += 1\n","        \n","        # TODO: Determine how many first layers of ResNet-50 to freeze\n","        child_counter = 0\n","        for child in self.model_resnet.children():\n","            if child_counter < params['frozenChildren']:\n","                for param in child.parameters():\n","                    param.requires_grad = False\n","            elif child_counter == params['frozenChildren']:\n","                children_of_child_counter = 0\n","                for children_of_child in child.children():\n","                    if children_of_child_counter < 3:\n","                        for param in children_of_child.parameters():\n","                            param.requires_grad = False\n","                    else:\n","                        children_of_child_counter += 1\n","            else:\n","                print(\"child \",child_counter,\" was not frozen\")\n","            child_counter += 1\n","        \n","        # Set ResNet-50's FCN as an identity mapping\n","        num_fc_in = self.model_resnet.fc.in_features\n","        self.model_resnet.fc = nn.Identity()\n","        \n","        # TODO: Design your own FCN\n","        self.fc1 = nn.Linear(num_fc_in, params['l1Tol2Features'], bias = 3) # from input of size num_fc_in to output of size ?\n","        self.bn1 = nn.BatchNorm1d(num_features=params['l1Tol2Features'])\n","        self.fc2 = nn.Linear(params['l1Tol2Features'], 3, bias = 3) # from hidden layer to 3 class scores\n","\n","    def forward(self,x):\n","        # TODO: Design your own network, implement forward pass here\n","        \n","        relu = nn.ReLU() # No need to define self.relu because it contains no parameters\n","        \n","        with torch.no_grad():\n","            features = self.model_resnet(x)\n","            \n","        x = self.fc1(features) # Activation are flattened before being passed to the fully connected layers\n","        x = self.bn1(x)\n","        x = relu(x)\n","        x = self.fc2(x)\n","        \n","        # The loss layer will be applied outside Network class\n","        return x\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n","model= Network().to(device)\n","criterion = nn.CrossEntropyLoss() # Specify the loss layer (note: CrossEntropyLoss already includes LogSoftMax())\n","# TODO: Modify the line below, experiment with different optimizers and parameters (such as learning rate)\n","optimizer = optim.AdamW(\n","    filter(lambda p: p.requires_grad, model.parameters()), \n","           lr = params['lr'], \n","           weight_decay = params['wtDecay'] # Specify optimizer and assign trainable parameters to it, weight_decay is L2 regularization strength (default: lr=1e-2, weight_decay=1e-4)\n",")\n","\n","num_epochs = params['epochs'] # TODO: Choose an appropriate number of training epochs\n","saveData = []\n","\n","def train(model, loader, num_epoch = num_epochs): # Train the model\n","    print(\"Start training...\")\n","    model.train() # Set the model to training mode\n","    for i in range(num_epoch):\n","        running_loss = []\n","        for batch, label in tqdm(loader, position=0, leave=True):\n","            batch = batch.to(device)\n","            label = label.to(device)\n","            optimizer.zero_grad() # Clear gradients from the previous iteration\n","            pred = model(batch) # This will call Network.forward() that you implement\n","            loss = criterion(pred, label) # Calculate the loss\n","            running_loss.append(loss.item())\n","            loss.backward() # Backprop gradients to all tensors in the network\n","            optimizer.step() # Update trainable weights\n","        print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n","        saveData.append((i+1, np.mean(running_loss)))\n","    print(\"Done!\")\n","\n","def evaluate(model, loader): # Evaluate accuracy on validation / test set\n","    model.eval() # Set the model to evaluation mode\n","    correct = 0\n","    with torch.no_grad(): # Do not calculate grident to speed up computation\n","        for batch, label in tqdm(loader, position=0, leave=True):\n","            batch = batch.to(device)\n","            label = label.to(device)\n","            pred = model(batch)\n","            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n","    acc = correct/len(loader.dataset)\n","    print(\"Evaluation accuracy: {}\".format(acc))\n","    return acc\n","    \n","train(model, trainloader, num_epochs)\n","print(\"Evaluate on test set\")\n","accuracy = evaluate(model, testloader)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["\r  0%|          | 0/420 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["child  6  was not frozen\n","child  7  was not frozen\n","child  8  was not frozen\n","child  9  was not frozen\n","Start training...\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:55<00:00,  7.54it/s]\n","  0%|          | 1/420 [00:00<00:54,  7.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1 loss:0.6355305328965187\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.46it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 2 loss:0.47046652336915334\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 3 loss:0.4087384464840094\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.43it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 4 loss:0.3584222853893325\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.45it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 5 loss:0.322721973522788\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.61it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 6 loss:0.2987854073445002\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.41it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 7 loss:0.2744016437480847\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.43it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.60it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 8 loss:0.26154548710300807\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 9 loss:0.24017005456345422\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.47it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 10 loss:0.2198370994645215\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.53it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 11 loss:0.21712721304169724\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.41it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.51it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 12 loss:0.202989505417645\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.58it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 13 loss:0.1968750740800585\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.40it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 14 loss:0.18519634267403967\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:57,  7.34it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 15 loss:0.18004251859549966\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.43it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 16 loss:0.17408080473542215\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.46it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.49it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 17 loss:0.15527231907028527\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.52it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 18 loss:0.15916698521801403\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.45it/s]\n","  0%|          | 1/420 [00:00<00:54,  7.62it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 19 loss:0.15494082163398465\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.41it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 20 loss:0.14236351267124217\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.46it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.55it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 21 loss:0.1433580495993651\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.45it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 22 loss:0.1428684377200192\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.43it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.46it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 23 loss:0.13498843875048416\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.42it/s]\n","  0%|          | 1/420 [00:00<00:56,  7.37it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 24 loss:0.1348080268307101\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.59it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 25 loss:0.1292732355317899\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 26 loss:0.12851453824190512\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.57it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 27 loss:0.11874202171429282\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.46it/s]\n","  0%|          | 1/420 [00:00<00:54,  7.63it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 28 loss:0.11934319964964829\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.44it/s]\n","  0%|          | 1/420 [00:00<00:55,  7.54it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 29 loss:0.11222827444961737\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 420/420 [00:56<00:00,  7.45it/s]\n","  1%|          | 1/101 [00:00<00:12,  8.11it/s]"],"name":"stderr"},{"output_type":"stream","text":["Epoch 30 loss:0.11924057688031878\n","Done!\n","Evaluate on test set\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 101/101 [00:12<00:00,  7.96it/s]"],"name":"stderr"},{"output_type":"stream","text":["Evaluation accuracy: 0.8609312365094048\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"S0dDB9m1plue","executionInfo":{"status":"ok","timestamp":1606168795640,"user_tz":600,"elapsed":757,"user":{"displayName":"Keenan Lee","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi8Wki9e0tZQDtTmt29KDphWhpTz1T0fjLgOarU6Q=s64","userId":"17155771245047856139"}}},"source":["import csv\n","accuracy = 0.8741905642923219\n","with open('/content/params.csv', 'w+') as store:\n","  storeWrite = csv.writer(store, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n","  #storeWrite.writerow([\"tnMean\", \"tstMean\", \"tnStd\", \"tstStd\", \"batch\", \"l1Tol2Features\", \"epochs\", \"lr\", \"wtDecay\", \"frozenChildren\"])\n","  for key, val in params.items():\n","    storeWrite.writerow([key, val])\n","  storeWrite.writerow(['epochNum', 'loss'])\n","  storeWrite.writerows(saveData)\n","  storeWrite.writerow(['Accuracy', accuracy])\n"],"execution_count":39,"outputs":[]}]}