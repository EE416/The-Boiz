{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/frendy/.local/lib/python3.6/site-packages (1.19.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/frendy/.local/lib/python3.6/site-packages (4.51.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /home/frendy/.local/lib/python3.6/site-packages (1.7.0)\n",
      "Requirement already satisfied: torchvision in /home/frendy/.local/lib/python3.6/site-packages (0.8.1)\n",
      "Requirement already satisfied: dataclasses in /home/frendy/.local/lib/python3.6/site-packages (from torch) (0.7)\n",
      "Requirement already satisfied: future in /home/frendy/.local/lib/python3.6/site-packages (from torch) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/frendy/.local/lib/python3.6/site-packages (from torch) (1.19.3)\n",
      "Requirement already satisfied: typing-extensions in /home/frendy/.local/lib/python3.6/site-packages (from torch) (3.7.4.3)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/frendy/.local/lib/python3.6/site-packages (from torchvision) (8.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: opencv-python in /home/frendy/.local/lib/python3.6/site-packages (4.4.0.44)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/frendy/.local/lib/python3.6/site-packages (from opencv-python) (1.19.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/frendy/.local/lib/python3.6/site-packages (1.1.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/frendy/.local/lib/python3.6/site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /home/frendy/.local/lib/python3.6/site-packages (from pandas) (1.19.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /home/frendy/.local/lib/python3.6/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/frendy/.local/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting h5py\n",
      "  Downloading h5py-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (4.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.0 MB 909 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cached-property; python_version < \"3.8\"\n",
      "  Downloading cached_property-1.5.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: numpy>=1.12; python_version == \"3.6\" in /home/frendy/.local/lib/python3.6/site-packages (from h5py) (1.19.3)\n",
      "Installing collected packages: cached-property, h5py\n",
      "Successfully installed cached-property-1.5.2 h5py-3.1.0\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Install dependency\n",
    "############\n",
    "!pip3 install numpy\n",
    "!pip3 install tqdm\n",
    "!pip3 install torch torchvision\n",
    "!pip3 install opencv-python\n",
    "!pip3 install pandas\n",
    "!pip3 install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "############\n",
    "# Numpy makes my computer run out of memory, fix it by forcing to not do multi threading\n",
    "# For Frendy Only\n",
    "###########\n",
    "import os\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '2'\n",
    "os.environ['MKL_NUM_THREADS'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old data\n",
      "Creating new directories\n",
      "Generating training CSV\n",
      "Generating testing CSV\n"
     ]
    }
   ],
   "source": [
    "#===========================\n",
    "# This is to split the data into training and testing\n",
    "# We also rotated the images\n",
    "# ==========================\n",
    "\n",
    "def splitData():\n",
    "    # Paths to dst directories \n",
    "    trainImage = os.path.normpath('./Dataset/Train/image')\n",
    "    testImage = os.path.normpath('./Dataset/Test/image')\n",
    "    trainLabel = os.path.normpath('./Dataset/Train/label')\n",
    "    testLabel = os.path.normpath('./Dataset/Test/label')\n",
    "    dtset = os.path.normpath('./Dataset')\n",
    "\n",
    "    trainingCSV = os.path.join(trainLabel, 'training.csv')\n",
    "    testingCSV = os.path.join(testLabel, 'testing.csv')\n",
    "    \n",
    "    rowTraining = []\n",
    "    rowTesting = []\n",
    "    \n",
    "    # To go through all files\n",
    "    index = 1\n",
    "    label = 0\n",
    "    \n",
    "    # Delete Dataset Folder\n",
    "    if os.path.exists(dtset):\n",
    "        print('Removing old data')\n",
    "        shutil.rmtree(dtset) \n",
    "\n",
    "    # Create directories\n",
    "    print('Creating new directories')\n",
    "    os.makedirs(trainImage)\n",
    "    os.makedirs(testImage)\n",
    "    os.makedirs(trainLabel)\n",
    "    os.makedirs(testLabel)\n",
    "    \n",
    "    # Dimensions for ResNet-50\n",
    "    dimW = 224\n",
    "    dimH = 224\n",
    "\n",
    "    # Process images\n",
    "    for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "        if 'Dataset' in dirpath:\n",
    "            continue\n",
    "        elif os.path.exists(os.path.join(dirpath, '_MACOSX')):\n",
    "            print('Removing _MACOSX dir')\n",
    "            shutil.rmtree(os.path.join(dirpath, '_MACOSX'))\n",
    "\n",
    "        # Get isCovid = 0, IsHealthy = 1 isOthers = 2   \n",
    "        if \"Covid\" in dirpath:\n",
    "            label = 0\n",
    "        elif \"Healthy\" in dirpath:\n",
    "            label = 1\n",
    "        elif 'Others'  in dirpath:\n",
    "            label = 2\n",
    "            \n",
    "        for filename in [f for f in filenames if f.endswith(\".png\")]: \n",
    "            # Read image\n",
    "            img = cv2.imread(os.path.join(dirpath, filename), cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            # Resize image\n",
    "            imgResized = cv2.resize(img, (dimW, dimH), interpolation=cv2.INTER_AREA)\n",
    "            # Convert to grayscale (faster processing)\n",
    "            imgGray = cv2.cvtColor(imgResized, cv2.COLOR_BGR2GRAY)\n",
    "            \n",
    "            # Split into 4:1\n",
    "            for i in range(4): \n",
    "                whereToGo = random.uniform(0, 1)\n",
    "                # Training\n",
    "                if whereToGo <= 0.80:\n",
    "                    # Add number and label \n",
    "                    rowTraining.append([index,label])\n",
    "                    cv2.imwrite(os.path.join(trainImage,str(index)+\".png\"), imgGray)\n",
    "                # Testing\n",
    "                else:\n",
    "                    # Add number and label \n",
    "                    rowTesting.append([index,label])   \n",
    "                    cv2.imwrite(os.path.join(testImage,str(index)+\".png\"), imgGray)\n",
    "                index += 1\n",
    "                # Rotate image\n",
    "                imgGray = cv2.rotate(imgGray, cv2.ROTATE_90_CLOCKWISE)\n",
    "                \n",
    "\n",
    "    # field names  \n",
    "    fields = ['Name', 'label']  \n",
    "    \n",
    "    # writing to trainingCSV file  \n",
    "    print('Generating training CSV')\n",
    "    with open(trainingCSV, 'w+') as csvfile:  \n",
    "        # creatittng a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields)  \n",
    "\n",
    "        # writing the data rows  \n",
    "        csvwriter.writerows(rowTraining)   \n",
    "        \n",
    "    # writing to trainingCSV file  \n",
    "    print('Generating testing CSV')\n",
    "    with open(testingCSV, 'w+') as csvfile:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields)  \n",
    "\n",
    "        # writing the data rows  \n",
    "        csvwriter.writerows(rowTesting)             \n",
    "    \n",
    "splitData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "class Data:\n",
    "\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.ROOT = root\n",
    "        self.images = self.read_images(root + \"/image\")\n",
    "        self.labels = self.read_labels(root + \"/label\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of points in the dataset\n",
    "\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Here we have to return the item requested by `idx`. The PyTorch DataLoader class will use this method to make an iterable for training/validation loop.\n",
    "\n",
    "        img = images[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    # Read Images\n",
    "    def read_images(self, path:str) -> list:\n",
    "        output = []\n",
    "        for file in sorted(os.listdir(path), key=lambda f : int(f[:-4])):\n",
    "            if file.endswith(\".png\"):\n",
    "                dir_path = os.path.join(path, file)\n",
    "                img = cv2.imread(dir_path)   \n",
    "                output.append(img)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    # Read Labels\n",
    "    def read_labels(self, path:str) -> list:\n",
    "        output = []\n",
    "        for file in os.listdir(path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                file = os.path.join(path, file)\n",
    "                df = pd.read_csv(file)\n",
    "                output = df.label #you can also use df['column_name']\n",
    "            \n",
    "        return output       \n",
    "\n",
    "# Load the dataset and train and test splits\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Data path\n",
    "TrainData = Data('./Dataset/Train')\n",
    "TestData = Data('./Dataset/Test')\n",
    "\n",
    "######################################################\n",
    "# Save data into binary files so we can just load them\n",
    "######################################################\n",
    "# Delete Dataset Folder\n",
    "dtpath = os.path.normpath('./Data')\n",
    "tnImg  = os.path.join(dtpath, 'Train/images')\n",
    "tnLbl  = os.path.join(dtpath, 'Train/labels')\n",
    "tstImg = os.path.join(dtpath, 'Test/images')\n",
    "tstLbl = os.path.join(dtpath, 'Test/labels')\n",
    "\n",
    "if os.path.exists():\n",
    "    print('Removing old data')\n",
    "    shutil.rmtree(dtset) \n",
    "\n",
    "# Create new directory\n",
    "os.makedirs(tnImg)\n",
    "os.makedirs(tnLbl)\n",
    "os.makedirs(tstImg)\n",
    "os.makedirs(tstLbl)\n",
    "\n",
    "# For TrainData\n",
    "# Images\n",
    "with h5py.File(os.path.join(tnImg, \"imagedata.hdf5\"), \"w\") as data_file:\n",
    "    for i,image in enumerate(TrainData.images):\n",
    "        data_file.create_dataset(str(i), data=image)\n",
    "# Labels\n",
    "with h5py.File(os.path.join(tnLbl, \"labeldata.hdf5\"), \"w\") as data_file:\n",
    "    data_file.create_dataset(\"labels\", data=TrainData.labels)\n",
    "\n",
    "# For TestData\n",
    "# Images\n",
    "with h5py.File(os.path.join(tstImg, \"imagedata.hdf5\"), \"w\") as data_file:\n",
    "    for i,image in enumerate(TestData.images):\n",
    "        data_file.create_dataset(str(i), data=image)\n",
    "# Labels\n",
    "with h5py.File(os.path.join(tstLbl, \"labeldata.hdf5\"), \"w\") as data_file:\n",
    "    data_file.create_dataset(\"labels\", data=TestData.labels)\n",
    "    \n",
    "# # Data normalization\n",
    "# MyTransform = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1), # Convert image to grayscale\n",
    "#     transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
    "#     transforms.Normalize([0.1], [0.2] ) # TODO: Normalize to zero mean and unit variance with appropriate parameters\n",
    "# ])\n",
    "\n",
    "# DATA_train = datasets.ImageFolder(root=DATA_train_path, transform=MyTransform)\n",
    "# DATA_test = datasets.ImageFolder(root=DATA_test_path, transform=MyTransform)\n",
    "\n",
    "# print(\"Done!\")\n",
    "\n",
    "# # Create dataloaders\n",
    "# # TODO: Experiment with different batch sizes\n",
    "trainloader = DataLoader(Data_train, batch_size=1, shuffle=True)\n",
    "testloader = DataLoader(Data_test, batch_size=2, shuffle=True)\n",
    "\n",
    "# print(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "\n",
    "    def __init__(self, root):\n",
    "        self.ROOT = root\n",
    "        self.images = self.read_images(os.path.join(root, \"images\"))\n",
    "        self.labels = self.read_labels(os.path.join(root, \"labels\"))\n",
    "\n",
    "        self.transform = MyTransform = transforms.Compose([\n",
    "            transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) # TODO: Normalize to zero mean and unit variance with appropriate parameters 0.5\n",
    "            ])\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of points in the dataset\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Here we have to return the item requested by `idx`. The PyTorch DataLoader class will use this method to make an iterable for training/validation loop.\n",
    "        img = self.transform(self.images[idx])\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n",
    "  \n",
    "    # Read Images\n",
    "    def read_images(self, path:str) -> list:\n",
    "        imgs = []\n",
    "        with h5py.File(os.path.join(path, \"imagedata.hdf5\"), \"r\") as f:\n",
    "            for i in sorted(f.keys(), key = int ):\n",
    "                imgs.append(np.array(f.get(str(i))))\n",
    "        #output = torch.tensor(output)\n",
    "        return imgs\n",
    "      \n",
    "    # Read Labels\n",
    "    def read_labels(self, path:str) -> list:\n",
    "        output = []\n",
    "        with h5py.File(os.path.join(path, \"labeldata.hdf5\"), \"r\") as f:\n",
    "            labels = f[\"labels\"][()]\n",
    "        return labels       \n",
    "\n",
    "# # Load the dataset and train and test splits\n",
    "# print(\"Loading datasets...\")\n",
    "\n",
    "# # Data path\n",
    "dataTrain = Data(os.path.normpath('./Data/Train'))\n",
    "dataTest  = Data(os.path.normpath('./Data/Test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13441\n",
      "13441\n"
     ]
    }
   ],
   "source": [
    "print(dataTrain.__len__())\n",
    "print(len(dataTrain.labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_train = datasets.ImageFolder(root=DATA_train_path, transform=MyTransform)\n",
    "# DATA_test = datasets.ImageFolder(root=DATA_test_path, transform=MyTransform)\n",
    "\n",
    "# print(\"Done!\")\n",
    "\n",
    "# # Create dataloaders\n",
    "# # TODO: Experiment with different batch sizes\n",
    "trainloader = DataLoader(dataTrain, batch_size=1, shuffle=True)\n",
    "testloader = DataLoader(dataTest, batch_size=2, shuffle=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/13441 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 11885/13441 [36:37<04:24,  5.89it/s] "
     ]
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: [Transfer learning with pre-trained ResNet-50] 1) Define how many first layers of convolutoinal neural network (CNN) feature extractor in ResNet-50 to be \"frozen\" and 2) design your own fully-connected network (FCN) classifier.\n",
    "        # 1) You will only refine last several layers of CNN feature extractor in ResNet-50 that mainly relate to high-level vision task. Determine how many first layers of ResNet-50 should be frozen to achieve best performances. Commented codes below will help you understand the architecture, i.e., \"children\", of ResNet-50.\n",
    "        # 2) Design your own FCN classifier. Here I provide a sample of two-layer FCN.\n",
    "        # Refer to PyTorch documentations of torch.nn to pick your layers. (https://pytorch.org/docs/stable/nn.html)\n",
    "        # Some common Choices are: Linear, ReLU, Dropout, MaxPool2d, AvgPool2d\n",
    "        # If you have many layers, consider using nn.Sequential() to simplify your code\n",
    "        \n",
    "        # Load pretrained ResNet-50\n",
    "        self.model_resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # The code below can show children of ResNet-50\n",
    "        #child_counter = 0\n",
    "        #for child in model.children():\n",
    "        #    print(\" child\", child_counter, \"is -\")\n",
    "        #    print(child)\n",
    "        #    child_counter += 1\n",
    "        \n",
    "        # TODO: Determine how many first layers of ResNet-50 to freeze\n",
    "        child_counter = 0\n",
    "        for child in self.model_resnet.children():\n",
    "            if child_counter < 47:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            elif child_counter == 47:\n",
    "                children_of_child_counter = 0\n",
    "                for children_of_child in child.children():\n",
    "                    if children_of_child_counter < 3:\n",
    "                        for param in children_of_child.parameters():\n",
    "                            param.requires_grad = False\n",
    "                    else:\n",
    "                        children_of_child_counter += 1\n",
    "            else:\n",
    "                print(\"child \",child_counter,\" was not frozen\")\n",
    "            child_counter += 1\n",
    "        \n",
    "        # Set ResNet-50's FCN as an identity mapping\n",
    "        num_fc_in = self.model_resnet.fc.in_features\n",
    "        self.model_resnet.fc = nn.Identity()\n",
    "        \n",
    "        # TODO: Design your own FCN\n",
    "        self.fc1 = nn.Linear(num_fc_in, 64, bias = 3) # from input of size num_fc_in to output of size ?\n",
    "        self.fc2 = nn.Linear(64, 3, bias = 3) # from hidden layer to 3 class scores\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO: Design your own network, implement forward pass here\n",
    "        \n",
    "        relu = nn.ReLU() # No need to define self.relu because it contains no parameters\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model_resnet(x)\n",
    "            \n",
    "        x = self.fc1(features) # Activation are flattened before being passed to the fully connected layers\n",
    "        x = relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # The loss layer will be applied outside Network class\n",
    "        return x\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n",
    "model= Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Specify the loss layer (note: CrossEntropyLoss already includes LogSoftMax())\n",
    "# TODO: Modify the line below, experiment with different optimizers and parameters (such as learning rate)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=.01, weight_decay=2) # Specify optimizer and assign trainable parameters to it, weight_decay is L2 regularization strength (default: lr=1e-2, weight_decay=1e-4)\n",
    "num_epochs =20 # TODO: Choose an appropriate number of training epochs\n",
    "\n",
    "def train(model, loader, num_epoch = num_epochs): # Train the model\n",
    "    print(\"Start training...\")\n",
    "    model.train() # Set the model to training mode\n",
    "    for i in range(num_epoch):\n",
    "        running_loss = []\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad() # Clear gradients from the previous iteration\n",
    "            pred = model(batch) # This will call Network.forward() that you implement\n",
    "            loss = criterion(pred, label) # Calculate the loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward() # Backprop gradients to all tensors in the network\n",
    "            optimizer.step() # Update trainable weights\n",
    "        print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
    "    print(\"Done!\")\n",
    "\n",
    "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(batch)\n",
    "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
    "    acc = correct/len(loader.dataset)\n",
    "    print(\"Evaluation accuracy: {}\".format(acc))\n",
    "    return acc\n",
    "    \n",
    "train(model, trainloader, num_epochs)\n",
    "print(\"Evaluate on test set\")\n",
    "evaluate(model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
