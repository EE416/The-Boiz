{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (1.18.5)\n",
      "Requirement already satisfied: tqdm in d:\\anaconda\\lib\\site-packages (4.47.0)\n",
      "Collecting torch\n",
      "  Using cached torch-0.1.2.post2.tar.gz (128 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.5.0-cp38-cp38-win_amd64.whl (1.2 MB)\n",
      "Requirement already satisfied: pyyaml in d:\\anaconda\\lib\\site-packages (from torch) (5.3.1)\n",
      "Requirement already satisfied: numpy in d:\\anaconda\\lib\\site-packages (from torchvision) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in d:\\anaconda\\lib\\site-packages (from torchvision) (7.2.0)\n",
      "Requirement already satisfied: six in d:\\anaconda\\lib\\site-packages (from torchvision) (1.15.0)\n",
      "Building wheels for collected packages: torch\n",
      "  Building wheel for torch (setup.py): started\n",
      "  Building wheel for torch (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for torch\n",
      "Failed to build torch\n",
      "Installing collected packages: torch, torchvision\n",
      "    Running setup.py install for torch: started\n",
      "    Running setup.py install for torch: finished with status 'error'\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'D:\\Anaconda\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d 'C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-wheel-dxl3bv03'\n",
      "       cwd: C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\\n",
      "  Complete output (30 lines):\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_deps\n",
      "  Traceback (most recent call last):\n",
      "    File \"<string>\", line 1, in <module>\n",
      "    File \"C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\setup.py\", line 225, in <module>\n",
      "      setup(name=\"torch\", version=\"0.1.2.post2\",\n",
      "    File \"D:\\Anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 165, in setup\n",
      "      return distutils.core.setup(**attrs)\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\core.py\", line 148, in setup\n",
      "      dist.run_commands()\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "      self.run_command(cmd)\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"D:\\Anaconda\\lib\\site-packages\\wheel\\bdist_wheel.py\", line 223, in run\n",
      "      self.run_command('build')\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\command\\build.py\", line 135, in run\n",
      "      self.run_command(cmd_name)\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "      self.distribution.run_command(command)\n",
      "    File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "      cmd_obj.run()\n",
      "    File \"C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\setup.py\", line 51, in run\n",
      "      from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n",
      "  ModuleNotFoundError: No module named 'tools.nnwrap'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed building wheel for torch\n",
      "  ERROR: Command errored out with exit status 1:\n",
      "   command: 'D:\\Anaconda\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' clean --all\n",
      "       cwd: C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\n",
      "  Complete output (2 lines):\n",
      "  running clean\n",
      "  error: [Errno 2] No such file or directory: '.gitignore'\n",
      "  ----------------------------------------\n",
      "  ERROR: Failed cleaning build dir for torch\n",
      "ERROR: torchvision 0.5.0 has requirement torch==1.4.0, but you'll have torch 0.1.2.post2 which is incompatible.\n",
      "    ERROR: Command errored out with exit status 1:\n",
      "     command: 'D:\\Anaconda\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-record-jn9pxcvv\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\Anaconda\\Include\\torch'\n",
      "         cwd: C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\\n",
      "    Complete output (23 lines):\n",
      "    running install\n",
      "    running build_deps\n",
      "    Traceback (most recent call last):\n",
      "      File \"<string>\", line 1, in <module>\n",
      "      File \"C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\setup.py\", line 225, in <module>\n",
      "        setup(name=\"torch\", version=\"0.1.2.post2\",\n",
      "      File \"D:\\Anaconda\\lib\\site-packages\\setuptools\\__init__.py\", line 165, in setup\n",
      "        return distutils.core.setup(**attrs)\n",
      "      File \"D:\\Anaconda\\lib\\distutils\\core.py\", line 148, in setup\n",
      "        dist.run_commands()\n",
      "      File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 966, in run_commands\n",
      "        self.run_command(cmd)\n",
      "      File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\setup.py\", line 99, in run\n",
      "        self.run_command('build_deps')\n",
      "      File \"D:\\Anaconda\\lib\\distutils\\cmd.py\", line 313, in run_command\n",
      "        self.distribution.run_command(command)\n",
      "      File \"D:\\Anaconda\\lib\\distutils\\dist.py\", line 985, in run_command\n",
      "        cmd_obj.run()\n",
      "      File \"C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-install-v_xpuoec\\torch\\setup.py\", line 51, in run\n",
      "        from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n",
      "    ModuleNotFoundError: No module named 'tools.nnwrap'\n",
      "    ----------------------------------------\n",
      "ERROR: Command errored out with exit status 1: 'D:\\Anaconda\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\keenl\\\\AppData\\\\Local\\\\Temp\\\\pip-install-v_xpuoec\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\keenl\\AppData\\Local\\Temp\\pip-record-jn9pxcvv\\install-record.txt' --single-version-externally-managed --compile --install-headers 'D:\\Anaconda\\Include\\torch' Check the logs for full command output.\n",
      "Requirement already satisfied: opencv-python in d:\\anaconda\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda\\lib\\site-packages (from opencv-python) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# Install dependency\n",
    "############\n",
    "!pip3 install numpy\n",
    "!pip3 install tqdm\n",
    "!pip3 install torch torchvision\n",
    "!pip3 install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing old data\n",
      "Creating new directories\n",
      "Finding max image size\n",
      "Max dimensions: 561 W, 468 H\n",
      "Generating training CSV\n",
      "Generating testing CSV\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import csv\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "def splitData():\n",
    "    # Paths to dst directories \n",
    "    trainImage = os.path.normpath('./Dataset/Train/image')\n",
    "    testImage = os.path.normpath('./Dataset/Test/image')\n",
    "    trainLabel = os.path.normpath('./Dataset/Train/label')\n",
    "    testLabel = os.path.normpath('./Dataset/Test/label')\n",
    "    dtset = os.path.normpath('./Dataset')\n",
    "\n",
    "    trainingCSV = os.path.join(trainLabel, 'training.csv')\n",
    "    testingCSV = os.path.join(testLabel, 'testing.csv')\n",
    "    \n",
    "    rowTraining = []\n",
    "    rowTesting = []\n",
    "    \n",
    "    # To go through all files\n",
    "    index = 1\n",
    "    label = 0\n",
    "    \n",
    "    # Delete Dataset Folder\n",
    "    if os.path.exists(dtset):\n",
    "        print('Removing old data')\n",
    "        shutil.rmtree(dtset) \n",
    "\n",
    "    # Create directories\n",
    "    print('Creating new directories')\n",
    "    os.makedirs(trainImage)\n",
    "    os.makedirs(testImage)\n",
    "    os.makedirs(trainLabel)\n",
    "    os.makedirs(testLabel)\n",
    "    \n",
    "    # Arbritrary large starting numbers\n",
    "    dimW = 0\n",
    "    dimH = 0\n",
    "\n",
    "    print('Finding max image size')\n",
    "    for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "        for filename in [f for f in filenames if f.endswith(\".png\")]:\n",
    "            # read image\n",
    "            img = cv2.imread(os.path.join(dirpath, filename), cv2.IMREAD_UNCHANGED)\n",
    "            if img is not None:\n",
    "                imgDim = img.shape\n",
    "            # store dimensions if larger than current largest\n",
    "            if imgDim[0] > dimH:\n",
    "                dimH = imgDim[0]\n",
    "            if imgDim[1] > dimW:\n",
    "                dimW = imgDim[1]           \n",
    "    print(f'Max dimensions: {dimW} W, {dimH} H')\n",
    "\n",
    "    # Process images\n",
    "    for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "        if 'Dataset' in dirpath:\n",
    "            continue\n",
    "        elif os.path.exists(os.path.join(dirpath, '_MACOSX')):\n",
    "            print('Removing _MACOSX dir')\n",
    "            shutil.rmtree(os.path.join(dirpath, '_MACOSX'))\n",
    "\n",
    "        # Get isCovid = 0, IsHealthy = 1 isOthers = 2   \n",
    "        if \"Covid\" in dirpath:\n",
    "            label = 0\n",
    "        elif \"Healthy\" in dirpath:\n",
    "            label = 1\n",
    "        elif 'Others'  in dirpath:\n",
    "            label = 2\n",
    "            \n",
    "        for filename in [f for f in filenames if f.endswith(\".png\")]: \n",
    "            # Resize image\n",
    "            img = cv2.imread(os.path.join(dirpath, filename), cv2.IMREAD_UNCHANGED)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            # Split into 4:1\n",
    "            # Training\n",
    "            for i in range(4):     \n",
    "                whereToGo = random.uniform(0, 1)\n",
    "                imgResized = cv2.resize(img, (dimW, dimH), interpolation=cv2.INTER_AREA)\n",
    "                if whereToGo <= 0.80:\n",
    "                    # Add number and label \n",
    "                    rowTraining.append([index,label])\n",
    "                    cv2.imwrite(os.path.join(trainImage,str(index)+\".png\"), imgResized)\n",
    "                # Testing\n",
    "                else:\n",
    "                    # Add number and label \n",
    "                    rowTesting.append([index,label])   \n",
    "                    cv2.imwrite(os.path.join(testImage,str(index)+\".png\"), imgResized)\n",
    "                index += 1 \n",
    "                img = cv2.rotate(img, cv2.cv2.ROTATE_90_CLOCKWISE)\n",
    "                \n",
    "                             \n",
    "\n",
    "    # field names  \n",
    "    fields = ['Name', 'label']  \n",
    "    \n",
    "    # writing to trainingCSV file  \n",
    "    print('Generating training CSV')\n",
    "    with open(trainingCSV, 'w+') as csvfile:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields)  \n",
    "\n",
    "        # writing the data rows  \n",
    "        csvwriter.writerows(rowTraining)   \n",
    "        \n",
    "    # writing to trainingCSV file  \n",
    "    print('Generating testing CSV')\n",
    "    with open(testingCSV, 'w+') as csvfile:  \n",
    "        # creating a csv writer object  \n",
    "        csvwriter = csv.writer(csvfile)  \n",
    "\n",
    "        # writing the fields  \n",
    "        csvwriter.writerow(fields)  \n",
    "\n",
    "        # writing the data rows  \n",
    "        csvwriter.writerows(rowTesting)             \n",
    "    \n",
    "splitData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dataset', '.ipynb_checkpoints', 'test', '.git']\n",
      "['Test', 'Train']\n",
      "['label', 'image']\n",
      "[]\n",
      "[]\n",
      "['label', 'image']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Others', 'Covid', 'Healthy']\n",
      "['Others']\n",
      "['Patient (9)', 'Patient (40)', 'Patient (35)', 'Patient (75)', 'Patient (59)', 'Patient (23)', 'Patient (12)', 'Patient (65)', 'Patient (60)', 'Patient (66)', 'Patient (27)', 'Patient (14)', 'Patient (41)', 'Patient (72)', 'Patient (34)', 'Patient (31)', 'Patient (51)', 'Patient (18)', 'Patient (48)', 'Patient (8)', 'Patient (29)', 'Patient (46)', 'Patient (5)', 'Patient (28)', 'Patient (56)', 'Patient (68)', 'Patient (76)', 'Patient (11)', 'Patient (4)', 'Patient (62)', 'Patient (71)', 'Patient (21)', 'Patient (42)', 'Patient (70)', 'Patient (55)', 'Patient (7)', 'Patient (67)', 'Patient (16)', 'Patient (73)', 'Patient (30)', 'Patient (61)', 'Patient (25)', 'Patient (10)', 'Patient (19)', 'Patient', 'Patient (50)', 'Patient (26)', 'Patient (79)', 'Patient (32)', 'Patient (36)', 'Patient (54)', 'Patient (3)', 'Patient (74)', 'Patient (52)', 'Patient (49)', 'Patient (80)', 'Patient (44)', 'Patient (22)', 'Patient (58)', 'Patient (33)', 'Patient (57)', 'Patient (20)', 'Patient (13)', 'Patient (69)', 'Patient (15)', 'Patient (37)', 'Patient (17)', 'Patient (63)', 'Patient (47)', 'Patient (64)', 'Patient (43)', 'Patient (77)', 'Patient (2)', 'Patient (53)', 'Patient (78)', 'Patient (24)', 'Patient (45)', 'Patient (39)', 'Patient (6)', 'Patient (38)']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Covid']\n",
      "['Patient (9)', 'Patient (40)', 'Patient (35)', 'Patient (75)', 'Patient (59)', 'Patient (23)', 'Patient (12)', 'Patient (65)', 'Patient (60)', 'Patient (66)', 'Patient (27)', 'Patient (14)', 'Patient (41)', 'Patient (72)', 'Patient (34)', 'Patient (31)', 'Patient (51)', 'Patient (18)', 'Patient (48)', 'Patient (8)', 'Patient (29)', 'Patient (46)', 'Patient (5)', 'Patient (28)', 'Patient (56)', 'Patient (68)', 'Patient (76)', 'Patient (11)', 'Patient (4)', 'Patient (62)', 'Patient (71)', 'Patient (21)', 'Patient (42)', 'Patient (70)', 'Patient (55)', 'Patient (7)', 'Patient (67)', 'Patient (16)', 'Patient (73)', 'Patient (30)', 'Patient (61)', 'Patient (25)', 'Patient (10)', 'Patient (19)', 'Patient', 'Patient (50)', 'Patient (26)', 'Patient (79)', 'Patient (32)', 'Patient (36)', 'Patient (54)', 'Patient (3)', 'Patient (74)', 'Patient (52)', 'Patient (49)', 'Patient (80)', 'Patient (44)', 'Patient (22)', 'Patient (58)', 'Patient (33)', 'Patient (57)', 'Patient (20)', 'Patient (13)', 'Patient (69)', 'Patient (15)', 'Patient (37)', 'Patient (17)', 'Patient (63)', 'Patient (47)', 'Patient (64)', 'Patient (43)', 'Patient (77)', 'Patient (2)', 'Patient (53)', 'Patient (78)', 'Patient (24)', 'Patient (45)', 'Patient (39)', 'Patient (6)', 'Patient (38)']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['Healthy']\n",
      "['Patient (9)', 'Patient (40)', 'Patient (35)', 'Patient (23)', 'Patient (12)', 'Patient (27)', 'Patient (14)', 'Patient (41)', 'Patient (34)', 'Patient (31)', 'Patient (18)', 'Patient (48)', 'Patient (8)', 'Patient (29)', 'Patient (46)', 'Patient (5)', 'Patient (28)', 'Patient (11)', 'Patient (4)', 'Patient (21)', 'Patient (42)', 'Patient (7)', 'Patient (16)', 'Patient (30)', 'Patient (25)', 'Patient (10)', 'Patient (19)', 'Patient', 'Patient (50)', 'Patient (26)', 'Patient (32)', 'Patient (36)', 'Patient (3)', 'Patient (49)', 'Patient (44)', 'Patient (22)', 'Patient (33)', 'Patient (20)', 'Patient (13)', 'Patient (15)', 'Patient (37)', 'Patient (17)', 'Patient (47)', 'Patient (43)', 'Patient (2)', 'Patient (24)', 'Patient (45)', 'Patient (39)', 'Patient (6)', 'Patient (38)']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['refs', 'hooks', 'info', 'logs', 'objects']\n",
      "['tags', 'heads', 'remotes']\n",
      "[]\n",
      "[]\n",
      "['origin']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['refs']\n",
      "['heads', 'remotes']\n",
      "[]\n",
      "['origin']\n",
      "[]\n",
      "['pack', 'c0', '8e', '24', '05', '6e', '67', 'a6', 'info', 'b8', 'cb', 'ca', 'e7', 'ec', '94', '2c', 'f4', 'c2', '19', 'c4', 'ad', '4a']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import os.path\n",
    "\n",
    "for dirpath, dirnames, filenames in os.walk(\".\"):\n",
    "    print (dirnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'read_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-726d0ebd2533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Data path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mDATA_train_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Dataset/Train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mDATA_test_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Dataset/Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-726d0ebd2533>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mROOT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/image\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/label\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_images' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm # Displays a progress bar\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, Subset, DataLoader, random_split\n",
    "\n",
    "# TODO: Construct your data in the following baseline structure: \n",
    "# 1) ./Dataset/Train/image/, \n",
    "# 2) ./Dataset/Train/label, \n",
    "# 3) ./Dataset/Test/image, and \n",
    "# 4) ./Dataset/Test/label\n",
    "class DataSet:\n",
    "\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.ROOT = root\n",
    "        self.images = read_images(root + \"/image\")\n",
    "        self.labels = read_labels(root + \"/label\")\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return number of points in the dataset\n",
    "\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Here we have to return the item requested by `idx`. The PyTorch DataLoader class will use this method to make an iterable for training/validation loop.\n",
    "\n",
    "        img = images[idx]\n",
    "        label = labels[idx]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "# Load the dataset and train and test splits\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "# Data path\n",
    "DATA_train_path = DataSet('./Dataset/Train')\n",
    "DATA_test_path = DataSet('./Dataset/Test')\n",
    "\n",
    "# # Data normalization\n",
    "# MyTransform = transforms.Compose([\n",
    "#     transforms.Grayscale(num_output_channels=1), # Convert image to grayscale\n",
    "#     transforms.ToTensor(), # Transform from [0,255] uint8 to [0,1] float\n",
    "#     transforms.Normalize([0.1], [0.2] ) # TODO: Normalize to zero mean and unit variance with appropriate parameters\n",
    "# ])\n",
    "\n",
    "# DATA_train = datasets.ImageFolder(root=DATA_train_path, transform=MyTransform)\n",
    "# DATA_test = datasets.ImageFolder(root=DATA_test_path, transform=MyTransform)\n",
    "\n",
    "# print(\"Done!\")\n",
    "\n",
    "# # Create dataloaders\n",
    "# # TODO: Experiment with different batch sizes\n",
    "# trainloader = DataLoader(Data_train, batch_size=1, shuffle=True)\n",
    "# testloader = DataLoader(Data_test, batch_size=2, shuffle=True)\n",
    "\n",
    "# print(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # TODO: [Transfer learning with pre-trained ResNet-50] 1) Define how many first layers of convolutoinal neural network (CNN) feature extractor in ResNet-50 to be \"frozen\" and 2) design your own fully-connected network (FCN) classifier.\n",
    "        # 1) You will only refine last several layers of CNN feature extractor in ResNet-50 that mainly relate to high-level vision task. Determine how many first layers of ResNet-50 should be frozen to achieve best performances. Commented codes below will help you understand the architecture, i.e., \"children\", of ResNet-50.\n",
    "        # 2) Design your own FCN classifier. Here I provide a sample of two-layer FCN.\n",
    "        # Refer to PyTorch documentations of torch.nn to pick your layers. (https://pytorch.org/docs/stable/nn.html)\n",
    "        # Some common Choices are: Linear, ReLU, Dropout, MaxPool2d, AvgPool2d\n",
    "        # If you have many layers, consider using nn.Sequential() to simplify your code\n",
    "        \n",
    "        # Load pretrained ResNet-50\n",
    "        self.model_resnet = models.resnet50(pretrained=True)\n",
    "        \n",
    "        # The code below can show children of ResNet-50\n",
    "        #child_counter = 0\n",
    "        #for child in model.children():\n",
    "        #    print(\" child\", child_counter, \"is -\")\n",
    "        #    print(child)\n",
    "        #    child_counter += 1\n",
    "        \n",
    "        # TODO: Determine how many first layers of ResNet-50 to freeze\n",
    "        child_counter = 0\n",
    "        for child in model_resnet.children():\n",
    "            if child_counter < ??:\n",
    "                for param in child.parameters():\n",
    "                    param.requires_grad = False\n",
    "            elif child_counter == ??:\n",
    "                children_of_child_counter = 0\n",
    "                for children_of_child in child.children():\n",
    "                    if children_of_child_counter < ???:\n",
    "                        for param in children_of_child.parameters():\n",
    "                            param.requires_grad = False\n",
    "                    else:\n",
    "                    children_of_child_counter += 1\n",
    "            else:\n",
    "                print(\"child \",child_counter,\" was not frozen\")\n",
    "            child_counter += 1\n",
    "        \n",
    "        # Set ResNet-50's FCN as an identity mapping\n",
    "        num_fc_in = self.model_resnet.fc.in_features\n",
    "        self.model_resnet.fc = nn.Identity()\n",
    "        \n",
    "        # TODO: Design your own FCN\n",
    "        self.fc1 = nn.Linear(num_fc_in, ?, bias = ??) # from input of size num_fc_in to output of size ?\n",
    "        self.fc2 = nn.Linear(?, 3, bias = ??) # from hidden layer to 3 class scores\n",
    "\n",
    "    def forward(self,x):\n",
    "        # TODO: Design your own network, implement forward pass here\n",
    "        \n",
    "        relu = nn.ReLU() # No need to define self.relu because it contains no parameters\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            features = self.model_resnet(x)\n",
    "            \n",
    "        x = self.fc1(features) # Activation are flattened before being passed to the fully connected layers\n",
    "        x = relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # The loss layer will be applied outside Network class\n",
    "        return x\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # Configure device\n",
    "model = Network().to(device)\n",
    "criterion = nn.CrossEntropyLoss() # Specify the loss layer (note: CrossEntropyLoss already includes LogSoftMax())\n",
    "# TODO: Modify the line below, experiment with different optimizers and parameters (such as learning rate)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=??, weight_decay=???) # Specify optimizer and assign trainable parameters to it, weight_decay is L2 regularization strength (default: lr=1e-2, weight_decay=1e-4)\n",
    "num_epochs = ?? # TODO: Choose an appropriate number of training epochs\n",
    "\n",
    "def train(model, loader, num_epoch = num_epochs): # Train the model\n",
    "    print(\"Start training...\")\n",
    "    model.train() # Set the model to training mode\n",
    "    for i in range(num_epoch):\n",
    "        running_loss = []\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            optimizer.zero_grad() # Clear gradients from the previous iteration\n",
    "            pred = model(batch) # This will call Network.forward() that you implement\n",
    "            loss = criterion(pred, label) # Calculate the loss\n",
    "            running_loss.append(loss.item())\n",
    "            loss.backward() # Backprop gradients to all tensors in the network\n",
    "            optimizer.step() # Update trainable weights\n",
    "        print(\"Epoch {} loss:{}\".format(i+1,np.mean(running_loss))) # Print the average loss for this epoch\n",
    "    print(\"Done!\")\n",
    "\n",
    "def evaluate(model, loader): # Evaluate accuracy on validation / test set\n",
    "    model.eval() # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    with torch.no_grad(): # Do not calculate grident to speed up computation\n",
    "        for batch, label in tqdm(loader):\n",
    "            batch = batch.to(device)\n",
    "            label = label.to(device)\n",
    "            pred = model(batch)\n",
    "            correct += (torch.argmax(pred,dim=1)==label).sum().item()\n",
    "    acc = correct/len(loader.dataset)\n",
    "    print(\"Evaluation accuracy: {}\".format(acc))\n",
    "    return acc\n",
    "    \n",
    "train(model, trainloader, num_epochs)\n",
    "print(\"Evaluate on test set\")\n",
    "evaluate(model, testloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
